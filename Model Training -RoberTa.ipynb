{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertForTokenClassification, BertTokenizerFast\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset and data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure data directory exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"data/ner_dataset.csv\"\n",
    "df = pd.read_csv(file_path, encoding=\"unicode_escape\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word Tag\n",
       "0  Sentence: 1      Thousands   O\n",
       "1          NaN             of   O\n",
       "2          NaN  demonstrators   O\n",
       "3          NaN           have   O\n",
       "4          NaN        marched   O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['Sentence #', 'Word', 'Tag']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    1000616\n",
       "Word               10\n",
       "Tag                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----  Missing value check ----------\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence: 1</th>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 10</th>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 100</th>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 1000</th>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 10000</th>\n",
       "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
       "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              Word   \n",
       "Sentence #                                                           \n",
       "Sentence: 1      [Thousands, of, demonstrators, have, marched, ...  \\\n",
       "Sentence: 10     [Iranian, officials, say, they, expect, to, ge...   \n",
       "Sentence: 100    [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "Sentence: 1000   [They, left, after, a, tense, hour-long, stand...   \n",
       "Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
       "\n",
       "                                                               Tag  \n",
       "Sentence #                                                          \n",
       "Sentence: 1      [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...  \n",
       "Sentence: 10     [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
       "Sentence: 100    [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...  \n",
       "Sentence: 1000                   [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "Sentence: 10000  [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Handling missing values\n",
    "\n",
    "# Fill missing \"Sentence #\" values (forward fill)\n",
    "df['Sentence #'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Drop missing words\n",
    "df.dropna(subset=['Word'], inplace=True)\n",
    "\n",
    "# Step 3: Replace missing tags with \"O\" (Outside entity)\n",
    "df['Tag'].fillna(\"O\", inplace=True)\n",
    "\n",
    "# Group words and tags into sentences\n",
    "grouped = df.groupby(\"Sentence #\").agg(lambda x: list(x))\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, testing and validation data\n",
    "\n",
    "train_data, test_data = train_test_split(grouped, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.125, random_state=42)  # 10% validation\n",
    "\n",
    "# Reset index\n",
    "train_data, val_data, test_data = train_data.reset_index(drop=True), val_data.reset_index(drop=True), test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Labels and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RoBERTa Tokenizer\n",
    "MODEL_NAME = \"roberta-base\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "\n",
    "\n",
    "# Create label mappings\n",
    "unique_tags = sorted(set(tag for tags in grouped[\"Tag\"] for tag in tags))\n",
    "tag2id = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "\n",
    "# Save label mappings\n",
    "import pickle\n",
    "# Ensure data directory exists\n",
    "os.makedirs(\"data_roberta\", exist_ok=True)\n",
    "\n",
    "# Save label mappings (Separate from BERT)\n",
    "with open(\"data_roberta/tag2id_roberta.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tag2id, f)\n",
    "with open(\"data_roberta/id2tag_roberta.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id2tag, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER Dataset Class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, tag2id, max_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = tag2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        words = self.data.iloc[index][\"Word\"]\n",
    "        tags = self.data.iloc[index][\"Tag\"]\n",
    "\n",
    "        # Tokenize and map labels\n",
    "        encoding = self.tokenizer(words, is_split_into_words=True, padding=\"max_length\",\n",
    "                                  truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        # Convert tags to IDs\n",
    "        tag_ids = [self.tag2id[tag] for tag in tags] + [self.tag2id[\"O\"]] * (self.max_len - len(tags))\n",
    "\n",
    "        encoding[\"labels\"] = torch.tensor(tag_ids[:self.max_len])\n",
    "\n",
    "        return {key: val.squeeze() for key, val in encoding.items()}  # Remove batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "train_dataset = NERDataset(train_data, tokenizer, tag2id)\n",
    "val_dataset = NERDataset(val_data, tokenizer, tag2id)\n",
    "test_dataset = NERDataset(test_data, tokenizer, tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/m8/p2450wkn6rzg_rq9f6lqvxqh0000gn/T/ipykernel_34883/1789436498.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25179' max='25179' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25179/25179 49:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.027024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.019479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.017674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa model training complete and saved at: models_roberta\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define model save path\n",
    "MODEL_PATH = \"models_roberta\"\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "# Load pre-trained RoBERTa for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(tag2id))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_PATH,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model separately\n",
    "model.save_pretrained(MODEL_PATH)\n",
    "tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n",
    "print(f\"RoBERTa model training complete and saved at: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barack', 'B-per', 0.9990480542182922), ('Obama', 'I-per', 0.9967117309570312), ('was', 'O', 0.9999501705169678), ('born', 'O', 0.9999850988388062), ('in', 'O', 0.999919056892395), ('Hawaii.', 'B-geo', 0.9960993528366089)]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the trained RoBERTa model for inference\n",
    "model_path = \"models_roberta\"\n",
    "ner_pipeline = pipeline(\"ner\", model=model_path, tokenizer=model_path)\n",
    "\n",
    "def predict_ner(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        max_probs, predictions = torch.max(probs, dim=-1)\n",
    "\n",
    "    # Load label mappings\n",
    "    with open(\"data_roberta/id2tag_roberta.pkl\", \"rb\") as f:\n",
    "        id2tag = pickle.load(f)\n",
    "\n",
    "    # Convert predictions to labels\n",
    "    predicted_tags = [id2tag[p] for p in predictions.squeeze().tolist()][:len(text.split())]\n",
    "    confidence_scores = max_probs.squeeze().tolist()[:len(text.split())]\n",
    "\n",
    "    return list(zip(text.split(), predicted_tags, confidence_scores))\n",
    "\n",
    "# Example Prediction\n",
    "test_sentence = \"Barack Obama was born in Hawaii.\"\n",
    "print(predict_ner(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")  # Force CPU usage\n",
    "\n",
    "# Move model to CPU\n",
    "model.to(device)\n",
    "\n",
    "# Example: Ensure tensors are also on CPU\n",
    "tokens = tokenizer(\"Barack Obama was born in Hawaii.\", return_tensors=\"pt\")\n",
    "tokens = {key: val.to(device) for key, val in tokens.items()}  # Move tensors to CPU\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_16',\n",
       "  'score': 0.99958795,\n",
       "  'index': 1,\n",
       "  'word': 'ĠWelcome',\n",
       "  'start': 0,\n",
       "  'end': 7},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9826774,\n",
       "  'index': 2,\n",
       "  'word': 'Ġto',\n",
       "  'start': 8,\n",
       "  'end': 10},\n",
       " {'entity': 'LABEL_16',\n",
       "  'score': 0.9995703,\n",
       "  'index': 3,\n",
       "  'word': 'ĠCalifornia',\n",
       "  'start': 11,\n",
       "  'end': 21},\n",
       " {'entity': 'LABEL_16',\n",
       "  'score': 0.9987747,\n",
       "  'index': 4,\n",
       "  'word': '.',\n",
       "  'start': 21,\n",
       "  'end': 22}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 2\n",
    "sample_sentence = \"Welcome to California.\"\n",
    "ner_results = ner_pipeline(sample_sentence)\n",
    "ner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barack', 'B-per'), ('Obama', 'I-per'), ('was', 'O'), ('born', 'O'), ('in', 'O'), ('Hawaii.', 'B-geo')]\n",
      "[('Elon', 'B-per'), ('Musk', 'I-per'), ('founded', 'O'), ('SpaceX', 'B-org'), ('in', 'O'), ('2002', 'B-tim'), ('in', 'O'), ('California.', 'B-geo')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Define RoBERTa Model Path\n",
    "MODEL_PATH = \"models_roberta\"  # Ensure this is the correct directory for your RoBERTa model\n",
    "\n",
    "# Load model and tokenizer (RoBERTa)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH).to(device)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, add_prefix_space=True)  # Required for RoBERTa\n",
    "\n",
    "# Load label mappings\n",
    "with open(\"data_roberta/id2tag_roberta.pkl\", \"rb\") as f:\n",
    "    id2tag = pickle.load(f)\n",
    "\n",
    "# Function to predict NER tags with RoBERTa\n",
    "def predict_ner(sentence):\n",
    "    tokens = loaded_tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens = {key: val.to(device) for key, val in tokens.items()}  # Ensure tensors are on the same device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**tokens)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "    # Convert predictions to labels\n",
    "    predicted_tags = [id2tag[p] for p in predictions][:len(sentence.split())]\n",
    "\n",
    "    return list(zip(sentence.split(), predicted_tags))\n",
    "\n",
    "# Example Prediction\n",
    "test_sentences = [\n",
    "    \"Barack Obama was born in Hawaii.\",\n",
    "    \"Elon Musk founded SpaceX in 2002 in California.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(predict_ner(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         art     0.3000    0.0566    0.0952        53\n",
      "         eve     0.4000    0.3429    0.3692        35\n",
      "         geo     0.8338    0.8813    0.8569      3792\n",
      "         gpe     0.9492    0.9354    0.9422      1517\n",
      "         nat     0.3333    0.4444    0.3810         9\n",
      "         org     0.7205    0.6895    0.7046      1929\n",
      "         per     0.7652    0.7689    0.7670      1683\n",
      "         tim     0.8498    0.8489    0.8493      2052\n",
      "\n",
      "   micro avg     0.8209    0.8262    0.8235     11070\n",
      "   macro avg     0.6440    0.6210    0.6207     11070\n",
      "weighted avg     0.8181    0.8262    0.8214     11070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Define RoBERTa Model Path\n",
    "MODEL_PATH = \"models_roberta\"  # Ensure this is the correct directory for your RoBERTa model\n",
    "\n",
    "# Load model and tokenizer (RoBERTa)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, add_prefix_space=True)  # Required for RoBERTa\n",
    "\n",
    "# Load label mappings\n",
    "with open(\"data_roberta/id2tag_roberta.pkl\", \"rb\") as f:\n",
    "    id2tag = pickle.load(f)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_ner(model, dataset, tokenizer, id2tag):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    true_labels, pred_labels = [], []\n",
    "\n",
    "    for example in dataset:\n",
    "        # Move inputs to the correct device\n",
    "        tokens = {key: val.unsqueeze(0).to(device) for key, val in example.items() if key != \"labels\"}\n",
    "        labels = example[\"labels\"].tolist()\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "        # Convert label IDs to tag names\n",
    "        true_labels.append([id2tag[label] for label in labels if label != -100])  # Ignore padding tokens\n",
    "        pred_labels.append([id2tag[pred] for pred in predictions if pred != -100])  # Ignore padding tokens\n",
    "\n",
    "    # Print the classification report\n",
    "    print(classification_report(true_labels, pred_labels, digits=4))\n",
    "\n",
    "# Evaluate on the validation dataset\n",
    "evaluate_ner(model, val_dataset, tokenizer, id2tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    " RoBERTa performed slightly better than BERT overall\n",
    "Biggest improvements in event (eve), organization (org), and geopolitical (gpe) entities\n",
    "Artifacts (art) still struggle due to low dataset representation\n",
    "RoBERTa performs slightly worse on nationality (nat) due to very low support (only 9 samples)\n",
    "\n",
    "\n",
    "RoBERTa is slightly better than BERT-cased, especially for event detection, organizations, and geopolitical entities. However, the improvement is not drastic—it mainly enhances rare entity recognition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
